{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the librarries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    try:\n",
    "        return 1.0/(1.0 + math.exp(-x))\n",
    "    except:\n",
    "        return 1.0/(1+abs(x))\n",
    "            \n",
    "sigmoid_v=np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Relu(x):\n",
    "    if x>=0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "Relu_v=np.vectorize(Relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self):  #class constructor to instialize the variables\n",
    "        self.weights={}\n",
    "        self.layer_no=1\n",
    "        self.prev_size=0\n",
    "        self.ans={}\n",
    "        self.delta={}\n",
    "        self.ERROR=[]\n",
    "        self.loss=0\n",
    "        self.cnt=0\n",
    "        \n",
    "    def addLayer(self,layer_size,input_size=0,output_size=0):\n",
    "        if input_size!=0:\n",
    "            self.weights[self.layer_no]=np.random.uniform(-1.0,1.0, size=(input_size,layer_size))/np.sqrt(2/(layer_size*input_size))\n",
    "\n",
    "            self.delta[self.layer_no]=np.zeros((input_size,layer_size))        \n",
    "            self.inputL=input_size\n",
    "            self.prev_size=layer_size\n",
    "            self.layer_no+=1\n",
    "        elif output_size!=0:\n",
    "            self.weights[self.layer_no]=np.random.uniform(-1.0,1.0,size=(self.prev_size,output_size))/np.sqrt(2/(self.prev_size*output_size))\n",
    "            self.delta[self.layer_no]=np.zeros((self.prev_size,output_size))\n",
    "            self.outputL=output_size\n",
    "            self.layer_no+=1\n",
    "        else:\n",
    "            self.weights[self.layer_no]=np.random.uniform(-1.0,1.0,size=(self.prev_size,layer_size))/np.sqrt(2/(self.prev_size*layer_size))\n",
    "            self.delta[self.layer_no]=np.zeros((self.prev_size,layer_size))\n",
    "            self.layer_no+=1\n",
    "            self.prev_size=layer_size\n",
    "    def printweights(self):\n",
    "        for i in range(1,self.layer_no):\n",
    "            print(self.weights[i])\n",
    "    def forwardprop(self,X,Y):\n",
    "        self.ans={}\n",
    "        self.f_no=2\n",
    "        self.ans[1]=X\n",
    "        while self.f_no<self.layer_no:\n",
    "            self.ans[self.f_no]=sigmoid_v(np.dot(self.ans[self.f_no-1],self.weights[self.f_no-1]))\n",
    "            self.f_no+=1\n",
    "        self.ans[self.f_no]=sigmoid_v(np.dot(self.ans[self.f_no-1],self.weights[self.f_no-1]))\n",
    "    def backprop(self,X,Y):\n",
    "        self.f_no=self.layer_no-1\n",
    "        self.prev_delta=np.subtract(self.ans[self.f_no+1],Y)\n",
    "        self.cnt+=np.sum(self.prev_delta)\n",
    "        self.ERROR.append(np.sum(np.square(self.prev_delta)))\n",
    "        while self.f_no>=1:\n",
    "            #print(self.f_no)\n",
    "            for i in range(self.ans[self.f_no].shape[0]):\n",
    "                #print(self.prev_delta)\n",
    "                #print(\"Vikhyat\")\n",
    "                self.delta[self.f_no][i,:]=self.delta[self.f_no][i,:] + self.prev_delta*self.ans[self.f_no][i]\n",
    "            self.prev_delta=np.dot(self.prev_delta,(self.weights[self.f_no]).T)\n",
    "            self.temp=np.ones((1,self.ans[self.f_no].shape[0]))\n",
    "            self.temp1=np.subtract(self.temp,self.ans[self.f_no])\n",
    "            self.prev_delta=np.multiply(self.ans[self.f_no],self.temp1)\n",
    "            self.prev_delta=np.multiply(self.prev_delta,self.temp1)\n",
    "            self.f_no-=1\n",
    "    def fit(self,X,Y,batch_size,no_of_epoch,learning_rate):\n",
    "        self.L=np.zeros((no_of_epoch))\n",
    "        self.min_loss=1\n",
    "        for i in range(no_of_epoch):\n",
    "            self.cnt=0\n",
    "            for k in range(0,X.shape[0]//batch_size):\n",
    "                index=np.random.randint(0,X.shape[0],size=(batch_size))\n",
    "                for j in index:\n",
    "                    self.forwardprop(X[j],Y[j])\n",
    "                    self.backprop(X[j],Y[j])\n",
    "                \n",
    "                for j in range(1,self.layer_no):\n",
    "                    self.weights[j]=self.weights[j]*(1-(learning_rate)/X.shape[0]) - (learning_rate*self.delta[j])/batch_size\n",
    "            print(\"Loss\",abs(self.cnt/Y.shape[0]))\n",
    "            self.loss=abs(self.cnt)/Y.shape[0]\n",
    "            self.L[i]=self.loss\n",
    "            self.min_loss=min(self.min_loss,self.loss)\n",
    "            \n",
    "        return self.min_loss\n",
    "    def plotloss(self):\n",
    "        plt.plot(self.L)\n",
    "        plt.show()\n",
    "    def summary(self):\n",
    "        l=[]\n",
    "        for i in range(1,self.layer_no):\n",
    "            l.append([i ,self.weights[i].shape[0],self.weights[i].shape[1]])\n",
    "        print(tabulate(l,headers=['Layer_Number','Input_Size','Number_of_Features']))\n",
    "    def predict(self,X):\n",
    "        self.predictions=np.zeros((X.shape[0],self.outputL))\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction=X[i]\n",
    "            for j in range(1,self.layer_no-1):\n",
    "                prediction=sigmoid_v(np.dot(prediction,self.weights[j]))\n",
    "            prediction=sigmoid_v(np.dot(prediction,self.weights[self.layer_no-1]))\n",
    "            self.predictions[i]=prediction\n",
    "        return self.predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vikhyat\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\users\\vikhyat\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.addLayer(6,input_size=11,output_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.addLayer(6,input_size=0,output_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.addLayer(1,input_size=0,output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.006951608907657818\n",
      "Loss 0.002408405860953602\n",
      "Loss 0.010723112064759942\n",
      "Loss 0.008450389883872934\n",
      "Loss 0.004953646882259448\n",
      "Loss 0.006809209689889826\n",
      "Loss 0.018205700068198896\n",
      "Loss 0.014858358531638319\n",
      "Loss 0.010951377318332787\n",
      "Loss 0.0184068162341115\n",
      "Loss 0.015105778485294045\n",
      "Loss 0.029066890094203428\n",
      "Loss 0.028621028234645875\n",
      "Loss 0.01766498660122039\n",
      "Loss 0.033701966354086045\n",
      "Loss 0.028321278958735072\n",
      "Loss 0.019065508338901943\n",
      "Loss 0.03578973711906504\n",
      "Loss 0.029516419650194945\n",
      "Loss 0.035474151592481774\n"
     ]
    }
   ],
   "source": [
    "error=n.fit(X_train,y_train,400,20,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=n.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class confusion_matrix:\n",
    "    def __init__(self):\n",
    "        self.pred=np.zeros((2,2))\n",
    "        \n",
    "    def cm(self,Y_test,Y_pred):\n",
    "        for i in range(Y_test.shape[0]):\n",
    "            if Y_test[i]==1 and Y_pred[i][0]>=0.5:\n",
    "                self.pred[1][1]+=1\n",
    "            elif Y_test[i]==0 and Y_pred[i][0]<0.5:\n",
    "                self.pred[0][0]+=1\n",
    "            elif Y_test[i]==1 and Y_pred[i][0]<0.5:\n",
    "                self.pred[1][0]+=1\n",
    "            elif Y_test[i]==0 and Y_pred[i][0]>=0.5:\n",
    "                self.pred[0][1]+=1\n",
    "        return self.pred\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix=confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "com=cmatrix.cm(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1482.,  113.],\n",
       "       [ 373.,   32.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
